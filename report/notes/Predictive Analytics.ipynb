{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed551717",
   "metadata": {},
   "source": [
    "# Predictive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f7ddb",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a8dc6",
   "metadata": {},
   "source": [
    "First we created a small data set for predicting the hourly usage of the bike fleet. Bike trip data is aggregated and the hourly demand is calculated. Location data is not part of the prediction_df since the hourly demand of the whole bike fleet needs to be determined. Information on station granularity is therefore not considered. The weather data is reduced to the columns max_temp and precip. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54522301",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8575b3b",
   "metadata": {},
   "source": [
    "We added additional features based on the analysis and clusering. These are for example seasonal features and lag features. We displayed the correlation of these features in the correlation matrix. We used a high correlation with the feature 'number of trips' for assesing features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244f324",
   "metadata": {},
   "source": [
    "## Grid Search Validation and Train-Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080f85f",
   "metadata": {},
   "source": [
    "To find the best combination of parameters for our models we are using `GridSearchCV`.  It executes an exhaustive search over all parameters. `GridSearchCV` splits the data into multiple training and test sets with the same percentage of samples. By default the `KFold` algorithm is used which assumes independent samples and is not suitable for our time series dataset. This is because time series data always has the same sort of dependency to data from the past.\n",
    "\n",
    "As shown in the graph, `KFold` algorithm would not ensure that the model works only on training data, instead, we are using the `TimeSeriesSplit` algorithm which is a drop-in replacement. `TimeSeriesSplit` ensures that a fitted model is only evaluated on \"future\" data.\n",
    "\n",
    "[report/images/timeseriessplit_algorithm.png]\n",
    "\n",
    "For a final evaluation after the grid search, especially for visualization purposes we cannot simply take the train/test samples that are generated using `TimeSeriesSplit`, because after the best parameters are found, the model is refitted on the whole dataset. Therefore we preserve a 30% share of our whole dataset for the final re-evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ae022",
   "metadata": {},
   "source": [
    "## Predictive models and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c2210",
   "metadata": {},
   "source": [
    "Three different regression models (Random Forest, K-nearest neighbors, and XGBoost) were used to predict an outcome based on a dataset. The models were first tested with default parameters, and then fine-tuned hyperparamaters using GridSearchCV. For performance evaluation we chose the R^2 score as an easy to understand measure. The score approximates how well the regression models approximate the real data points. Deviation in model performance was low with around 1%. The K-Nearest Neighbors model showed the best performance with an RÂ² value of 0.956773. \n",
    "\n",
    "/bild_knn_vs_actual_data\n",
    "\n",
    "Model Name|\tUnoptimized|\tOptimized\n",
    "---|---|---|\n",
    "|RandomForestRegressor|\t0.931206|**0.931425**\n",
    "|KNeighborsRegressor|\t0.920491|0.927662\n",
    "|XGBRFRegressor|\t0.909585|\t0.930707\n",
    "\n",
    "The most important features for the prediction were found to be the lag values from one week before, as well as the \"is_workday\" feature. Temperature and precipitation were found to have little impact on the prediction. Since K-Nearest Neigbors does not allow feature importance analysis, the values were calculated based on XGBRFRegressor.\n",
    "\n",
    "/bild_feature_importance\n",
    "\n",
    "Further improvement could be achieved by using enhanced weather data for the features. The mild climate in San Francisco with low precipitation and temperature ranges allowing riding a bike all year long have a little influence on the demand. Possible enhancements could be achieved through wind strength and direction since San Francisco's location in a bay area.\n",
    " \n",
    "In case of the selected prediction models, improvements can be achieved by an advanced hyperparameter tuning. Our computational resources are limited and especially tuning the XGBoost Hyperparameters is cost-intensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa836b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
